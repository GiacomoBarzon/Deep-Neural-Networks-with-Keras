{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hOkbXW1Mniot"
   },
   "source": [
    "## Deep Neural Networks with Keras\n",
    "\n",
    "Barzon Giacomo 1207274 <br>\n",
    "Kabbur Hanumanthappa Manjunatha Karan 1236383 <br>\n",
    "Rugel Wolfgang 1218362 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# reproducibility\n",
    "np.random.seed(12345)\n",
    "\n",
    "# digits 1....9\n",
    "D = 9\n",
    "\n",
    "# loading the database\n",
    "fname = 'secretkeys_exe.csv'\n",
    "dataset = np.loadtxt(fname, delimiter=\",\", dtype=int) \n",
    "N = len(dataset) \n",
    "print(f'data: {N}')\n",
    "\n",
    "# extract data and labels\n",
    "s = dataset[:,0]\n",
    "y = dataset[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmenting the data by including all the L-1 cyclic combinations of the data.\n",
    "def augment(dataset):\n",
    "    s = dataset[:,0]\n",
    "    y = dataset[:,-1]\n",
    "    L = len(str(s[0]))\n",
    "    augmented_list = []\n",
    "    for i in range(len(s)):\n",
    "        augmented_list.append([s[i],y[i]])\n",
    "        string = str(s[i])\n",
    "        for j in range(L-1):\n",
    "            rotate = int(string[-1]+string[:-1])\n",
    "            string = str(rotate)\n",
    "            augmented_list.append([rotate,y[i]])\n",
    "    return augmented_list\n",
    "\n",
    "augmented_array = np.array(augment(dataset))\n",
    "print(augmented_array[:10])\n",
    "    \n",
    "# Randomly permute the data\n",
    "augmented_array = np.random.permutation(augmented_array)\n",
    "augmented_array = np.random.permutation(augmented_array)\n",
    "print(augmented_array[:10])\n",
    "\n",
    "# Save .txt file\n",
    "np.savetxt('Randomized_Augmented_secretkeys.csv',augmented_array,delimiter = ',',fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Grid-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a DNN class\n",
    "# input: grid-search parameters\n",
    "# output: .png with train/test curves, .h5 with best weights, best model test/train accuracy \n",
    "\n",
    "class myDNN():\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        self.D = 9 # digits 1...9\n",
    "        \n",
    "        self.activation = params[0]\n",
    "        self.optimizer = params[1]\n",
    "        self.lrate = params[2]\n",
    "        self.batch_size = params[3]\n",
    "        self.lreg = params[4]\n",
    "        self.winit = params[5]\n",
    "        self.dropout = params[6]\n",
    "        \n",
    "        self.loadData()\n",
    "        self.initNetwork()\n",
    "        \n",
    "        \n",
    "    def loadData(self):\n",
    "        \n",
    "        # Load csv dataset\n",
    "        data = 'Randomized_Augmented_secretkeys.csv'\n",
    "        data = np.loadtxt(data, delimiter=',',dtype=int)\n",
    "        \n",
    "        # Extract labels\n",
    "        labels = data[:,-1]\n",
    "        \n",
    "        # Extract samples and perform one-hot encoding\n",
    "        samples = data[:,0].astype(str)\n",
    "        self.L = len(str(samples[0])) # number of digits for eachs string\n",
    "        samples = [int(i) for j in range(samples.shape[0]) for i in samples[j]] # expand string\n",
    "        samples = np.array(samples).reshape(-1,self.L) -1\n",
    "        samples = to_categorical(samples).reshape(-1,self.L*self.D) # one-hot encoding\n",
    "        \n",
    "        # Split samples into train and test set\n",
    "        Ntrain = int(0.8 * labels.shape[0])\n",
    "        self.x_train, self.x_test = samples[:Ntrain], samples[Ntrain:]\n",
    "        self.y_train, self.y_test = labels[:Ntrain], labels[Ntrain:]\n",
    "        \n",
    "        \n",
    "    def initNetwork(self):\n",
    "        inputDim = self.L * self.D # samples shape\n",
    "        \n",
    "        # define Keras model\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(inputDim, input_shape=(inputDim,), activation=self.activation, \n",
    "                             kernel_regularizer=l2(self.lreg), bias_regularizer=l2(self.lreg),\n",
    "                            kernel_initializer=self.winit))\n",
    "        self.model.add(Dropout(self.dropout))\n",
    "        self.model.add(Dense(max(10,int(inputDim/2)), activation=self.activation, \n",
    "                             kernel_regularizer=l2(self.lreg), bias_regularizer=l2(self.lreg),\n",
    "                            kernel_initializer=self.winit))\n",
    "        self.model.add(Dropout(self.dropout))\n",
    "        self.model.add(Dense(max(6,int(inputDim/4)), activation=self.activation, \n",
    "                             kernel_regularizer=l2(self.lreg), bias_regularizer=l2(self.lreg),\n",
    "                            kernel_initializer=self.winit))\n",
    "        self.model.add(Dropout(self.dropout))\n",
    "        self.model.add(Dense(1, activation='sigmoid'))\n",
    "        #print(self.model.summary())\n",
    "        \n",
    "        # define optimizer\n",
    "        if self.optimizer == 'SGD':\n",
    "            opt = SGD(lr=self.lrate, decay=1e-6, momentum=0.7, nesterov=True)\n",
    "        elif self.optimizer == 'RMSprop':\n",
    "            opt = RMSprop(learning_rate=self.lrate)\n",
    "        elif self.optimizer == 'Adam':\n",
    "            opt = Adam(learning_rate=self.lrate)\n",
    "        \n",
    "        # compile model\n",
    "        self.model.compile(loss='binary_crossentropy',\n",
    "                           optimizer=opt,\n",
    "                           metrics=['accuracy'])\n",
    "        \n",
    "        \n",
    "    def train(self, printGraphs=True, verbose=False):\n",
    "        print('START TRAINING MODEL')\n",
    "        \n",
    "        # filename for saving best model\n",
    "        filename = 'a' + str(self.activation) + '_o' + str(self.optimizer) + '_l' + str(self.lrate)\n",
    "        filename += '_b' + str(self.batch_size) +'_r' + str(self.lreg) + '_w' + str(self.winit)\n",
    "        filename += '_d' + str(self.dropout)\n",
    "        weights_file = filename + '.h5'\n",
    "        \n",
    "        # implement early-stopping\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "        \n",
    "        # implement checkpoint for returning the best model\n",
    "        mc = ModelCheckpoint(weights_file,\n",
    "                             monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
    "        \n",
    "        # train model\n",
    "        fit = self.model.fit(self.x_train, self.y_train, validation_data=(self.x_test, self.y_test),\n",
    "                            epochs = 200, batch_size = self.batch_size, callbacks=[es, mc],\n",
    "                            verbose=verbose)\n",
    "        \n",
    "        # load the best model\n",
    "        self.model = load_model(weights_file)\n",
    "        \n",
    "        # evaluate the model\n",
    "        _, self.train_acc = self.model.evaluate(self.x_train, self.y_train, verbose=0)\n",
    "        _, self.test_acc = self.model.evaluate(self.x_test, self.y_test, verbose=0)\n",
    "        print('Train accuracy: %.3f, Test accuracy: %.3f' % (self.train_acc, self.test_acc))\n",
    "        \n",
    "        # save learning curves\n",
    "        graph_file = filename + '.png'\n",
    "        \n",
    "        fig = plt.figure(figsize=(15,6))\n",
    "        \n",
    "        # accuracy\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(fit.history['accuracy'])\n",
    "        plt.plot(fit.history['val_accuracy'])\n",
    "        plt.legend(['Training Accuracy', 'Validation Accuracy'])\n",
    "        plt.xlabel('Epochs ')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Accuracy Curves')\n",
    "        plt.grid(linestyle='--',alpha=0.5)\n",
    "\n",
    "        # loss\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(fit.history['loss'])\n",
    "        plt.plot(fit.history['val_loss'])\n",
    "        plt.legend(['Training loss', 'Validation Loss'])\n",
    "        plt.xlabel('Epochs ')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss Curves')\n",
    "        plt.grid(linestyle='--',alpha=0.5)\n",
    "\n",
    "        plt.savefig(graph_file)\n",
    "        plt.close(fig)\n",
    "        \n",
    "        return fit, self.train_acc, self.test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the hyperparameters for the grid-search\n",
    "from itertools import product\n",
    "\n",
    "gridSearch = {\n",
    "    \"activation\": ['relu', 'tanh', 'sigmoid'],\n",
    "    \"optimizer\": ['SGD', 'RMSprop', 'Adam'],\n",
    "    \"lrate\": [0.1, 0.01, 0.001],\n",
    "    \"batch_size\": [16, 32, 64],\n",
    "    \"lreg\": [0.001, 0.0001, 0.00001],\n",
    "    \"weight_init\": ['orthogonal', 'glorot_uniform', 'he_normal'],\n",
    "    \"dropout\": [0.2, 0.4]\n",
    "}\n",
    "\n",
    "gridSearch = list(product(*gridSearch.values()))\n",
    "\n",
    "n_models = len(gridSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run the grid-search\n",
    "train_acc, test_acc = [], []\n",
    "\n",
    "i = 0\n",
    "for params in gridSearch:\n",
    "    i += 1\n",
    "    print('Training model ' + str(i) + ' of ' + str(n_models))\n",
    "    print('Params: ' + str(params))\n",
    "    nn = myDNN(params)\n",
    "    _, temp_train_acc, temp_test_acc = nn.train()\n",
    "    \n",
    "    train_acc.append(temp_train_acc)\n",
    "    test_acc.append(temp_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create .txt log \n",
    "log_path = 'log_book.txt'\n",
    "\n",
    "log_text=open(log_path, \"w\")\n",
    "log_text.write(\"Params, TrainAcc, TestAcc\\n\")\n",
    "for i in range(len(train_acc)):\n",
    "    log_text.write(str(gridSearch[i])+', ' + str(train_acc[i]) + ', ' + str(test_acc[i])+'\\n')\n",
    "log_text.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose and retranspose (N x L*D) data matrix\n",
    "def transpose(x):\n",
    "    x_transpose=[[None]*N for i in range(LD)]\n",
    "    for i in range(N):\n",
    "        for j in range(LD):\n",
    "            x_transpose[j][i]=x[i][j]\n",
    "    x_transpose=np.array(x_transpose)\n",
    "    return x_transpose\n",
    "\n",
    "def retranspose(x_transpose):\n",
    "    x=[[None]*LD for i in range(N)]\n",
    "    for i in range(N):\n",
    "        for j in range(LD):\n",
    "            x[i][j]=x_transpose[j][i]\n",
    "    x=np.array(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "data = 'Randomized_Augmented_secretkeys.csv'\n",
    "data = np.loadtxt(data, delimiter=',',dtype=int)\n",
    "#length constants\n",
    "N=len(data)\n",
    "L=7\n",
    "D=9\n",
    "LD=L*D\n",
    "\n",
    "#Preprocessing data\n",
    "# Extract labels\n",
    "labels = data[:,-1]\n",
    "# Extract samples and perform one-hot encoding\n",
    "samples = data[:,0].astype(str)\n",
    "L = len(str(samples[0])) # number of digits for eachs string\n",
    "samples = [int(i) for j in range(samples.shape[0]) for i in samples[j]] # expand string\n",
    "samples = np.array(samples).reshape(-1,L) -1\n",
    "samples = to_categorical(samples).reshape(-1,L*D) # one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RESCALING\n",
    "#transpose all positions indicating the same digit in array\n",
    "x=samples\n",
    "x_transpose=transpose(x)\n",
    "\n",
    "#center each digit by the mean\n",
    "raw_center=[x_transpose[i]-x_transpose[i].mean() for i in range(LD)]\n",
    "#rescaling not necessary since all values already in [-1,1]\n",
    "x_transpose=np.array(raw_center)\n",
    "\n",
    "#retranspose to the original shape\n",
    "x_rescaled=retranspose(x_transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split samples into train and test set\n",
    "Ntrain = int(0.8 * labels.shape[0])\n",
    "x_train, x_test = samples[:Ntrain], samples[Ntrain:]\n",
    "y_train, y_test = labels[:Ntrain], labels[Ntrain:]\n",
    "\n",
    "x_scaled_train, x_scaled_test = x_rescaled[:Ntrain], x_rescaled[Ntrain:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL DEFINITION\n",
    "#choice Hyperparameter\n",
    "params=('tanh', 'SGD', 0.1, 32, 0.00001, 'glorot_uniform', 0.2)\n",
    "# translator\n",
    "activation = params[0]\n",
    "optimizer = params[1]\n",
    "lrate = params[2]\n",
    "batch_size = params[3]\n",
    "lreg = params[4]\n",
    "winit = params[5]\n",
    "dropout = params[6]\n",
    "\n",
    "def compile_model():\n",
    "    # instantiate model\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(inputDim, input_shape=(inputDim,), activation=activation, \n",
    "                     kernel_regularizer=l2(lreg), bias_regularizer=l2(lreg),\n",
    "                    kernel_initializer=winit))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(max(10,int(inputDim/2)), activation=activation, \n",
    "                         kernel_regularizer=l2(lreg), bias_regularizer=l2(lreg),\n",
    "                        kernel_initializer=winit))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(max(6,int(inputDim/4)), activation=activation, \n",
    "                         kernel_regularizer=l2(lreg), bias_regularizer=l2(lreg),\n",
    "                        kernel_initializer=winit))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    #optimizer\n",
    "    opt = SGD(lr=lrate, decay=1e-6, momentum=0.7, nesterov=True)\n",
    "        \n",
    "    # compile the model\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                   optimizer=opt,\n",
    "                   metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING\n",
    "inputDim = L * D # samples shape\n",
    "\n",
    "#### unscaled data ####\n",
    "#model\n",
    "model_unscaled=compile_model()\n",
    "# train\n",
    "start1=time.time()\n",
    "history=model_unscaled.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=200,\n",
    "          verbose=False,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "#### rescaled data ####\n",
    "model_rescaled=compile_model()\n",
    "start2=time.time()\n",
    "history1=model_rescaled.fit(x_scaled_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=200,\n",
    "          verbose=False,\n",
    "          validation_data=(x_scaled_test, y_test))\n",
    "print('time unscaled', start2-start1)\n",
    "print('time rescaled', time.time()-start2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOT\n",
    "fig = plt.figure(figsize=(15,6))\n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "plt.plot(history.history['accuracy'],color='#1f77b4',label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'],color='#2ca02c',label='Validation Accuracy')\n",
    "plt.plot(history1.history['accuracy'],linestyle='--',color='#ff7f0e', label='Training Accuracy rescaled')\n",
    "plt.plot(history1.history['val_accuracy'],linestyle='--',color='#d62728', label='Validation Accuracy rescaled')\n",
    "plt.legend(loc='best', bbox_to_anchor=(0.5, 0.35, 0.5, 0.75),prop={'size':10})\n",
    "plt.xlabel('Epochs ')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('FIG 1A: Accuracy Curves')\n",
    "plt.grid(linestyle='--',alpha=0.5)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "\n",
    "plt.plot(history.history['loss'],color='#1f77b4',label='Training loss')\n",
    "plt.plot(history.history['val_loss'],color='#2ca02c',label='Validation loss')\n",
    "plt.plot(history1.history['loss'],linestyle='--',color='#ff7f0e',label='Training loss rescaled')\n",
    "plt.plot(history1.history['val_loss'],linestyle='--',color='#d62728',label='validation loss rescaled')\n",
    "plt.legend(loc=\"best\",prop={'size':10})\n",
    "plt.xlabel('Epochs ')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('FIG 1B: Loss Curves')\n",
    "plt.grid(linestyle='--',alpha=0.5)\n",
    "plt.legend\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
